





from IPython.display import Image
display(Image("cover.png"))



























































from IPython.display import Image
metadata = {
    "jdh": {
        "module": "object",
        "object": {
            "source": [
                "The temporal distribution of news sheets in the MIA-Euronews Corpus"
            ],
            "type":
            "image"
        }
    }
}
display(Image('media/image19.png'), metadata=metadata)


metadata = {
    "jdh": {
        "module": "object",
        "object": {
            "source": [
                "The fifteen most frequent news hubs in the transcribed news sheet corpus in the MIA-Euronews Corpus"
            ],
            "type":
            "image"
        }
    }
}
display(Image('media/image6.png'),metadata=metadata)


















































Image('media/image28.png')


from scipy.stats import beta
import numpy as np
from matplotlib import pyplot as plt
import os
from scipy.stats import bernoulli, binom
import pandas as pd
figure, axis = plt.subplots(1,2,figsize=(15, 4))
X = bernoulli(p=0.3)
X_samples = X.rvs(100)
entries = []
entry = {'outcome': 'Head', 'count':np.where(X_samples==1)[0].shape[0]}
entries.append(entry)
entry = {'outcome': 'Tail', 'count':np.where(X_samples==0)[0].shape[0]}
entries.append(entry)
dfTmp =  pd.DataFrame(entries)
dfTmp.plot.bar(x='outcome',y='count',ax=axis[0],width=0.25)
axis[0].set_ylim(ymin=0,ymax = 100)
axis[0].set_xlabel('Outcome')
axis[0].set_ylabel('Count')
axis[0].set(title='A')
for f,p in enumerate(axis[0].patches):
    if f == 0:
        axis[0].annotate(str(p.get_height()), (p.get_x() * 0.3, p.get_height() * 1.15))
    else:
        axis[0].annotate(str(p.get_height()), (p.get_x() * 1.1, p.get_height() * 1.08))

x = np.linspace(0, 1, 1002)[1:-1]
a = dfTmp.iloc()[0]['count']
b = dfTmp.iloc()[1]['count']
dist = beta(a+1, b+1)
mean = beta.mean(a+1, b+1)
meanY = dist.pdf(mean)
axis[1].plot(x, dist.pdf(x))
axis[1].set(title='D: Beta distribution of Coin flipping')
axis[1].vlines(x = mean, ymin = 0, ymax = meanY, colors = 'black', label = 'Mean value' ,linestyle = 'dotted')
axis[1].set_xlabel('Probability')
axis[1].set_ylabel('Probability density')
axis[1].set(title='B')
axis[1].set_xlim(0,1)
axis[1].set_ylim(0,10)
ax2 = axis[1].twiny()

labels = [item.get_text() for item in ax2.get_xticklabels()]
labels[0]="Head"
labels[-1]="Tail"
ax2.tick_params(axis='x', colors='red')
ax2.set_xticklabels(labels)
axis[1].set_xticks([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9])
figure.tight_layout()






















































import json
from msmtools.estimation import transition_matrix,count_matrix, is_connected
from pyrandwalk import *
import pandas as pd

# Load the data

f = open('data/input/newsSheetsEncoded.json')
finalDocs = json.load(f)
finalDocs = [np.array(element) for element in finalDocs]

f = open('data/input/dictionaryMarkovianSimulation.json')
dictionary = json.load(f)

f = open('data/input/actorsTerms.json')
actorsTerms = json.load(f)

# Create a count matrix

def createCountMatrix(trajectories,dictionary,window=1):
    countMatrix = np.zeros((len(dictionary),len(dictionary)))
    for i in range(1,window+1):
        countMatrix = countMatrix + count_matrix(trajectories,lag=i)
    countMatrix = np.array(countMatrix)
    assert len(dictionary) == countMatrix.shape[0]
    return countMatrix

cm = createCountMatrix(finalDocs,dictionary,5)
assert len(np.where(cm.sum(axis=1)==0)[0]) == 0

# Create the transition matrix

def createTransitionMatrix(trajectories,dictionary,window=1):
    countMatrix = np.zeros((len(dictionary),len(dictionary)))
    for i in range(1,window+1):
        countMatrix = countMatrix + count_matrix(trajectories,lag=i)
    countMatrix = np.array(countMatrix)
    P = transition_matrix(countMatrix)
    assert len(dictionary) == P.shape[0]
    return P

P = createTransitionMatrix(finalDocs,dictionary,window=5)

# Train the Markov chain and get the statitionary probabilityof each word

def printStableProbabilities(P,dictionary):
    assert len(P) == len(dictionary)
    vocabNumerical = [i for i,element in enumerate(dictionary)]
    rw = RandomWalk(vocabNumerical, P)
    stationaryProbs = rw.final_dist()
    dfStationaryProb = pd.DataFrame(stationaryProbs,columns=['prob'])
    dfStationaryProb['term'] = dictionary
    dfStationaryProb['scaledToMedian'] = dfStationaryProb['prob'] / dfStationaryProb['prob'].median()
    dfStationaryProb.sort_values('prob',ascending=False,inplace=True)
    return dfStationaryProb

stationaryProb = printStableProbabilities(P,dictionary)
stationaryProb = stationaryProb.sort_values('prob',ascending=False)

# Print the stationary probability of key actors, as well as whether they are part of the upper quartile

dfTemp = stationaryProb.reset_index()
quantile = []
for element in dfTemp.prob.to_list():
    if element > dfTemp.prob.quantile(0.75):
        quantile.append(True)
    else:
        quantile.append(False)
dfTemp['upperQuantile'] = quantile
dfTemp[dfTemp.term.isin(actorsTerms)]














Image('media/image12.png',width=800)


Image('media/image25.png')


Image('media/image14.png')


from scipy.stats import beta

def dt_parse(s):
    y,m,d = s.split('-')
    return pd.Period(year=int(y), month=int(m), day=int(d), freq='D')


df = pd.read_csv('data/input/WomenMenAbsencePresence.csv')
df['date']=df.date.apply(dt_parse)
df = df.set_index('date')


n_trials = 100
sampleSize = 100
finalResults = []
temporalBoundaries = [1600,1599]
for f,time in enumerate(temporalBoundaries):
    results = []
    for i in range(0,n_trials):
        if f == 0:
            #print (df[(df.index.year<time)].shape)
            sample = df[(df.index.year<time)].sample(n=sampleSize)
        else:
            #print (df[(df.index.year>time)].shape)
            sample = df[(df.index.year>time)].sample(n=sampleSize)
        success = sample[sample.woman==True].shape[0]
        failure = sample[sample.woman==False].shape[0]
        result = np.array([success,failure])
        results.append(result)
    results = np.vstack(results)
    a = results.mean(axis=0)[0]
    b = results.mean(axis=0)[1]
    dist = beta(a+1, b+1)
    X = np.linspace(0, 1, 1002)[1:-1]
    y = dist.pdf(X)
    # Confidence levels

    lower, upper = beta.interval(0.95, a+1, b+1, loc=0)
    meanY = dist.pdf(mean)
    lowerY = dist.pdf(lower)
    upperY = dist.pdf(upper)
    a = np.format_float_positional(a+1,precision=2)
    b = np.format_float_positional(b+1,precision=2)
    if f == 0:
        titleText = 'A: Before 1600 ('+r'$\alpha$='+str(a)+r', $\beta$='+str(b)+')'
    else:
        titleText = 'B: After 1600 ('+r'$\alpha$='+str(a)+r', $\beta$='+str(b)+')'

    finalResult = {'title':titleText,
                   'a':a,'b':b,
                   'x':X,'y':y,
                   'meanX':dist.mean(),
                   'meanY':dist.pdf(dist.mean()),
                   'lowerY':lowerY,
                   'lowerX':lower,
                   'upperY':upperY,
                   "upperX":upper}

    finalResults.append(finalResult)  

### Plot it

A = np.array(finalResults)
B = np.reshape(A, (-1, 2))

figure, axis = plt.subplots(B.shape[0], B.shape[1],figsize=(15, 4))

for i in range(B.shape[0]):
    for f in range(B.shape[1]):
        axis[f].plot(B[i][f]['x'], B[i][f]['y'],label='Beta distribution of terms denoting women')
        axis[f].set_title(B[i][f]['title'])
        axis[f].vlines(x = B[i][f]['meanX'],ymax= B[i][f]['meanY'],ymin = 0,linestyle = 'dotted',label='Mean value',colors = 'black')
        axis[f].vlines(x = B[i][f]['upperX'],ymax= B[i][f]['upperY'],ymin = 0,linestyle = 'dashed',label='Confidence level',colors = 'black')
        axis[f].vlines(x = B[i][f]['lowerX'],ymax= B[i][f]['lowerY'],ymin = 0,linestyle = 'dashed',colors = 'black')
        axis[f].set_ylim(ymin=0,ymax=13)
        axis[f].set_xlim(xmin=0,xmax=1)

        axis[f].set_ylabel(r'Probability density')
        axis[f].set_xlabel(r'Probability')



handles, labels = axis[f].get_legend_handles_labels()
figure.legend(handles, labels,bbox_to_anchor=(0.9, 1.2), loc='upper center', borderaxespad=0)


plt.show()




Image('media/image36.png')


from scipy.stats import beta

dfTemp = df.docid.value_counts().to_frame('docid')
indices = dfTemp[dfTemp.docid>7].index.to_list()
df = df[df.docid.isin(indices)]


# Sample women
alpha_values = []
beta_values = []
n_trials = 100
sampleSize = 100
results = []
for i in range(0,n_trials):
    #sample = df.sample(n=sampleSize)

    sample = df.docid.sample(n=sampleSize)

    resultM = []
    resultW = []
    for dId in sample.to_list():
        resW = True in df[df.docid==dId].woman.to_list()
        resM = True in df[df.docid==dId].man.to_list()
        resultW.append(resW)
        resultM.append(resM)

    # Postprocess sample
    sample = sample.to_frame()
    sample['woman'] = resultW


    success = sample[sample.woman==True].shape[0]
    failure = sample[sample.woman==False].shape[0]
    result = np.array([success,failure])
    results.append(result)
results = np.vstack(results)
a = results.mean(axis=0)[0]
b = results.mean(axis=0)[1]
alpha_values.append(a)
beta_values.append(b)


means = []
linestyles = ['#4682B4']
x = np.linspace(0, 1, 1002)[1:-1]
labels = ['Beta distribution of terms denoting women','Beta distribution of terms denoting men']

#------------------------------------------------------------
# plot the distributions
fig, ax = plt.subplots(figsize=(5, 3.75))
num = 0
for a, b, ls, label in zip(alpha_values[0:1], beta_values[0:1], linestyles[0:1],labels[0:1]):
    lower, upper = beta.interval(0.95, a+1, b+1, loc=0)
    mean = beta.mean(a+1, b+1)

    dist = beta(a+1, b+1)
    meanY = dist.pdf(mean)
    lowerY = dist.pdf(lower)
    upperY = dist.pdf(upper)
    plt.plot(x, dist.pdf(x), c=ls,
             label=label)
    if num == 1:
        plt.vlines(x = lower, ymin = 0, ymax = lowerY, colors = 'black' ,linestyle = 'dashed')
        plt.vlines(x = upper, ymin = 0, ymax = upperY, colors = 'black', label = 'Confidence level' ,linestyle = 'dashed')
        plt.vlines(x = mean, ymin = 0, ymax = meanY, colors = 'black', label = 'Mean value' ,linestyle = 'dotted')
    else:
        plt.vlines(x = lower, ymin = 0, ymax = lowerY, colors = 'black' ,linestyle = 'dashed',label = 'Confidence level')
        plt.vlines(x = mean, ymin = 0, ymax = meanY, colors = 'black', linestyle = 'dotted',label = 'Mean value')
        plt.vlines(x = upper, ymin = 0, ymax = upperY, colors = 'black' ,linestyle = 'dashed')
    print("Mean:"+str(mean))
    print ("Positive deviation from the mean:"+str(upper-mean))
    print ("Negative deviation from the mean:"+str(mean-lower))
    means.append(dist.mean())
    num +=1
plt.xlim(0, 1)
plt.ylim(0, 15)



plt.xlabel('Probability')
plt.ylabel(r'Probability density')
a_W = np.format_float_positional(alpha_values[0]+1,precision=2)
b_W = np.format_float_positional(beta_values[0]+1,precision=2)

titleTextW = 'Women ('+r'$\alpha$='+str(a_W)+r', $\beta$='+str(b_W)+')'

titleText = titleTextW
plt.title(titleText)

plt.legend(bbox_to_anchor=(2,1.2), borderaxespad=1)










# Get those words the stationary probability of which is in the upper quartile
keyTerms = stationaryProb[stationaryProb.prob>stationaryProb.prob.quantile(0.75)]

# Print those that belong to a certain group of topics, including their stationary probability
dfKeyTerms = pd.read_csv('data/input/keyTermsTopics.csv',delimiter=';')
dfKeyTerms = dfKeyTerms.groupby('Topic')['Term'].apply(list).to_frame()
for row in dfKeyTerms.iterrows():
    print('Topic - '+row[0]+':\n')
    for word in row[1]['Term']:
        try:
            print(word)
            prob = keyTerms[keyTerms.term==word]['prob'].values[0]
        except:
            continue
    print('\n')




# Print those words that are the most likely to accompany terms related to women

womenTerms = ['regina',
 'signora',
 'contessa',
 'marchesa',
 'duchessa',
 'principessa',
 'sposa',
 'moglie',
 'madre',
 'consorte',
 'sorella',
 'figlia',
 'donna',
 'nipote',
 'gentildonna']

terms = pd.read_csv('data/input/keyWordsSurroundingWomen.csv',delimiter=';')
addendum = ['vestire','tagliare','difesa',
           'prigione','attaccare','assedio','sorella','figlia','figlio','deliberare','entrata','moglie']
for el in addendum:
    entry = pd.DataFrame([{'Term':el}])
    terms = pd.concat([terms, entry], ignore_index = True, axis = 0)
terms = terms[~terms.Term.isin(['povero','peste','dire','ducato','condurre','celebrare','visita','robbe'])]

# First run the simulation of the corpus

def simulateText(P,dictionary,length):
    vocabNumerical = [i for i,element in enumerate(dictionary)]
    rw = RandomWalk(vocabNumerical, P)
    states, probs = rw.run(ntimes=length)
    return states

states = simulateText(P,dictionary,1000000)

# Get those terms that accompany women terms

def postProcessSimulationResultsV2(probabilities,term,stationaryProb):
    entries = []
    for key in probabilities:
        try:
            entry = {'term':key,'prob':probabilities[key][-1]}
        except:
            entry = {'term':key,'prob':np.nan}
        entries.append(entry)

    df = pd.DataFrame(entries).sort_values('prob',ascending=False)
    df['scaledToMedian'] = df['prob']/df.prob.median()

    p_a = []
    for ter in term:
        prob = stationaryProb[stationaryProb['term']==ter]['prob'].values[0]
        p_a.append(prob)
    p_a = np.array(p_a).sum()
    results = []
    for row in df.iterrows():
        term = row[1]['term']
        p_b_a = row[1]['prob']
        if np.isnan(p_b_a):
            p_a_b = np.nan
        else:
            p_b = stationaryProb[stationaryProb['term']==term]['prob'].values[0]
            p_a_b = bayes_theorem(p_a,p_b,p_b_a)
        results.append(p_a_b)
    df['bayesRescaled'] = results
    return df

def getContextualProbabilitiesV2(simulatedStates,term,dictionary,direction=None):

    counter = {element:[] for element in dictionary}
    probabilities = {element:[] for element in dictionary}
    means = {element:[] for element in dictionary}
    positions = {element:[] for element in dictionary}

    if direction == 'preceding':
        for i,element in enumerate(np.where(np.array(simulatedStates) == dictionary.index(term))[0]):
            try:
                neighbour = simulatedStates[element-1]
                p = len(counter[dictionary[neighbour]])/i
                probabilities[dictionary[neighbour]].append(p)
                means[dictionary[neighbour]].append(np.array(probabilities[dictionary[neighbour]]).mean())
                counter[dictionary[neighbour]].append(1)
                position = element
                positions[dictionary[neighbour]].append(position)
            except Exception as e:
                #print(e)
                pass
        return means, positions
    elif direction == 'following':
        for i,element in enumerate(np.where(np.array(simulatedStates) == dictionary.index(term))[0]):
            try:
                neighbour = simulatedStates[element+1]
                p = len(counter[dictionary[neighbour]])/i

                probabilities[dictionary[neighbour]].append(p)
                means[dictionary[neighbour]].append(np.array(probabilities[dictionary[neighbour]]).mean())
                counter[dictionary[neighbour]].append(1)
                position = element
                positions[dictionary[neighbour]].append(position)
            except Exception as e:
                #print(e)
                pass
        return means, positions

    elif direction == None:
        indices = []
        for ter in term:
            index = dictionary.index(ter)
            indices.append(index)
        locations = []

        for ind in indices:
            loc = np.where(np.array(simulatedStates) == ind)[0]
            locations.extend(loc)

        for i,element in enumerate(locations):
            try:
                neighbour = simulatedStates[element+1]
                p = len(counter[dictionary[neighbour]])/i

                probabilities[dictionary[neighbour]].append(p)
                means[dictionary[neighbour]].append(np.array(probabilities[dictionary[neighbour]]).mean())
                counter[dictionary[neighbour]].append(1)
                position = element
                positions[dictionary[neighbour]].append(position)
            except Exception as e:
                #print(e)
                pass
            try:
                neighbour = simulatedStates[element-1]
                p = len(counter[dictionary[neighbour]])/i

                probabilities[dictionary[neighbour]].append(p)
                means[dictionary[neighbour]].append(np.array(probabilities[dictionary[neighbour]]).mean())
                counter[dictionary[neighbour]].append(1)
                position = element
                positions[dictionary[neighbour]].append(position)
            except Exception as e:
                #print(e)
                pass
        return means, positions

    else:
        raise Exception("Direction should be either preceding or following or None")


def bayes_theorem(p_a, p_b, p_b_given_a):
  # calculate P(A|B) = P(B|A) * P(A) / P(B)
  p_a_given_b = (p_b_given_a * p_a) / p_b
  return p_a_given_b        

probabilities,positions = getContextualProbabilitiesV2(states,womenTerms,dictionary)
dfResult = postProcessSimulationResultsV2(probabilities,womenTerms,stationaryProb)
dfTemp = dfResult[dfResult.term.isin(terms.Term.to_list())]
dfTemp = dfTemp.fillna(0)

# Print those selected terms that are most likely to follow or precede women terms
display(dfTemp.sort_values('prob',ascending=False)[['prob','term']])

# Print the selected terms above with the strength of association between them and women terms

dfTemp['bayesRescaled'] = dfTemp.bayesRescaled/dfTemp.bayesRescaled.sum()
display(dfTemp.sort_values('bayesRescaled',ascending=False)[['prob','term']])


Image('media/image8.png',width=800)


Image('media/image15.png')


Image('media/image32.png')


Image('media/image29.png')


Image('media/image21.png')


Image('media/image20.png')


Image('media/image30.png')


Image('media/image24.png')


Image('media/image26.png')


Image('media/image10.png')


Image('media/image9.png')


Image('media/image5.png')





























































































































































































































































































